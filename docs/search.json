[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CASA0023 Learning Diary",
    "section": "",
    "text": "About\nHi, I’m Lama, I come from a background in architecture and urban planning. Following my undergraduate studies, I pursued an MSc in Construction Science and Implementation Techniques, in our dissertation, my partners and I innovated a novel clay material characterized by its sustainability and remarkable durability, capable of withstanding substantial tension and pressure forces. This achievement contributed in earning a distinction degree. During my previous master’s program, I worked as a senior architect, overseeing the design and construction of buildings across various functions. Additionally, I actively contributed to the regeneration and planning efforts in multiple areas within my country. Driven by my interest in the transformative potential of urban planning and the role of smart cities in societal advancement, I decided to pursue an MSc in Urban Spatial Science at UCL. I’m enthusiastic about exploring how data can be leveraged to create and enrich the fabric of our communities, so this is me and here I’m.\nBeside Architecture, urban planning and construction I really love taking photos , capturing beautiful moments, and travailing around the world\n\n\n\nMe on of the construction site (on the corner of a flying canopy)\n\n\n\n\n\nRichmond park Deers this was captured by me."
  },
  {
    "objectID": "Week 1.html",
    "href": "Week 1.html",
    "title": "1  Week 1 Remote sensing: an Arial adventure",
    "section": "",
    "text": "References\nEvaluation and comparison of Sentinel-2 MSI, Landsat 8 OLI, and EFFIS data for forest fires mapping. Illustrations from the summer 2017 fires in Tunisia.\nhttps://www.tandfonline.com/doi/full/10.1080/10106049.2021.1980118\nSENTINEL-2 MISSION GUIDE.\nhttps://sentinels.copernicus.eu/web/sentinel/missions/sentinel-2\nLANDSAT 8\nhttps://landsat.gsfc.nasa.gov/satellites/landsat-8/\nSatellites capture socioeconomic disruptions during the 2022 full‑scale war in Ukraine.\nhttps://www.nature.com/articles/s41598-023-42118-w\nWhat is Remote Sensing?\nhttps://www.earthdata.nasa.gov/learn/backgrounders/remote-sensing"
  },
  {
    "objectID": "Week 1.html#summary",
    "href": "Week 1.html#summary",
    "title": "1  Week 1 Remote sensing: an Arial adventure",
    "section": "1.1 Summary",
    "text": "1.1 Summary\nIn this lecture, we delved into the world of remotely sensing cities, and learned about the methods used to collect data using sensors that are mounted on various of platforms (space-borne/airborne). We explored the two main types of sensors: passive, which detect natural radiation, and active, which emit their own radiation. We also explored the differences between Sentinel 2 and Landsat satellites in terms of spectral bands, spatial resolution, and revisit frequency. Finally, we discussed that electromagnetic radiation can be obstructed by surfaces or the atmosphere, causing absorption, transmission, or scattering.\n\n\n\nFigure (1): Active and Passive sensors. website: (https://www.balamis.com/technology/)\n\n\n\n1.1.1 Remote sensing definition\nAccording to NASA “Remote sensing is the acquiring of information from a distance”. By observing the energy response (reflected, transmitted) and recording information then use it in responding to hazards, apply energy studies utilizing, urban planning, and environmental treaty enforcement…..etc.\n\n\n1.1.2 The process of remote sensing\nElectromagnetic energy, created by moving charged particles, travels in waves through space and the atmosphere. These waves vary in length and frequency; shorter waves have higher frequencies. Electromagnetic waves might get obstructed by a surface or atmosphere which cause scattering for the short wave such as blues and then it reflects back to be recorded and processed to information/data.\n\n\n\nFigure (2): the remote sensing process website: (https://paititi.info/research-technology/remote-sensing-from-space/)\n\n\n\n\n1.1.3 Sentinel-2 VS Landsat\nSentinel-2 The SENTINEL-2 mission consists of two satellites orbiting in the same path around the Earth, but spaced 180° apart. They’re designed to continuously observe changes in land surface conditions. With a wide coverage area and a rapid revisit time of 10 days at the equator.\nLandsat 8 The Landsat 8 satellite carries two main instruments: the Operational Land Imager (OLI) and the Thermal Infrared Sensor (TIRS). These sensors capture images of the Earth’s surface at various resolutions, including 30 meters for visible, near-infrared, and short-wave infrared data; 100 meters for thermal data; and 15 meters for panchromatic data"
  },
  {
    "objectID": "Week 1.html#application",
    "href": "Week 1.html#application",
    "title": "1  Week 1 Remote sensing: an Arial adventure",
    "section": "1.2 Application",
    "text": "1.2 Application\nLandsat and Sentinel 2 have been used extensively in different fields including land use planning and monitoring, emergency response and management, water use monitoring and others. Each one of those two methods has its strengths and to understand those strengths, we will leverage an analysis that was conducted to evaluate and compare between those two data sources in forest fires mapping article. This analysis compares and assesses spectral indices derived from Sentinel-2 and Landsat-8 OLI imagery and to identify the most appropriate iindex for each sensor for accurately mapping forest fires, using as a case study two fire events that occurred in summer 2017 in northern Tunisia (Achour et al. 2022, Toujani et al. 2022).\n\n\n\nFigure (2): Sentinel 2A Multi Spectral Instrument (MSI) and Landsat-8 Operational Land Imager (OLI) satellite images were acquired before (pre-fire) and after (post-fire) the fires from the U.S. Geological Survey (USGS) website (https://earthexplorer.usgs.gov/).\n\n\n\n1.2.1 Discussion\nResults from this study highlight those spectral indices including NIR-SWIR bands derived from Landsat 8 OLI and Sentinel-2 MSI (ΔNBRn, ΔNBR and RBR) had higher discriminatory power than classical indices based on NIR and red bands (BAI, ΔNDVI, ΔEVI, and ΔMSAVI).\nIn the figure below, we can see that spectral indices derived from Landsat 8, especially those combining the Red-NIR bands such as ΔNDVI, ΔEVI and ΔMSAVI exhibited a higher spectral separability as compared to their counterparts generated from Sentinel bands in both fires.\n\n\n\nFigure (3): M-statistic values to discern burned/unburned areas for both Sentinel 2 (S) and Landsat-8 OLI (L) spectral indices\n\n\nMaps of the estimated burned areas from each spectral index and for each fire reveals that both Landsat 8 and Sentinel-2A can capture the boundaries of fires identified using Copernicus EMS reference maps, albeit with small differences between burned areas, retrieved from each sensor.\nSentinel-based spectral indices (ΔNBRn and RBR) provided the highest level of positional accuracy, with a loc value of about 60 metres. This means that the positions of the extracted fires were shifted approximately by three pixels from their positions in the reference map. However, Landsat-based spectral indices, RBR and ΔNBR, displayed a loc value of about 75 meters, suggesting that the position of the extracted fires were shifted approximately 2.5 pixels from their positions in the reference map. The advantage of Sentinel 2A in terms of positional accuracy could be attributed to its higher spatial resolution as compared to Landsat 8.\n\n\n\nFigure (4): Distribution of burned and unburned values for spectral indices selected for F1 / F2\n\n\n\n\n\nFigure (5): Spectral indices vs. Copernicus EMS fire polygon for the Haddada fire\n\n\n\n\n1.2.2 Conclusion\nThe study concluded that (1) regardless of the sensor, spectral indices that incorporated NIR-SWIR bands exceed those using red and NIR bands in terms of spectral separability; (2) from the viewpoint of accuracy, the Sentinel sensor is slightly more efficient than the Landsat 8 in mapping burned scars, but both sensors produce similar and acceptable results. Hence, this paper concludes that both sensors are a good alternative to EFFIS data, particularly when there is a need to detect details inside the fire. I still have to add so what and why it is useful:"
  },
  {
    "objectID": "Week 1.html#reflection",
    "href": "Week 1.html#reflection",
    "title": "1  Week 1 Remote sensing: an Arial adventure",
    "section": "1.3 Reflection",
    "text": "1.3 Reflection\nThis lecture wasn’t easy to digest at first, as there was a lot of new ideas and terms (Spectral, Spectrum, Spectral Signature…etc ) to learn. when I went through the lecture it was captivating the facts that we learned for example the scattering of EMR and how the process of gathering satellite images is not as easy and simple as i thought, plus it was really interesting knowing why the sky and sea are blue. This lecture opened my eyes to the uses of satellite data and the importance of it and how it can be used in different areas and in very different methods but what was really interesting for me is the paper that i found on the use of satellite images in capturing the socioeconomic disruption during the Ukrainian war. In this study they combined satellite data on nitrogen dioxide (NO2) and carbon dioxide (CO2) to track changes in human activities, as both gases are linked to fossil fuel combustion. the analysis reveals a significant decrease in NO2 levels over major Ukrainian cities, power plants, and industrial areas during the second quarter of 2022, ranging from 15% to 46% compared to the reference period of 2018-2021. They also used the detection of unusual fire which might be likley from shelling rather than agricultural burning.These findings demonstrate the value of satellite observations in monitoring significant societal changes, particularly during conflicts."
  },
  {
    "objectID": "week 2.html#summary",
    "href": "week 2.html#summary",
    "title": "2  Week 2 Xaringan: Crafting Stunning Presentations",
    "section": "2.1 Summary",
    "text": "2.1 Summary\nThe content of week 2 was about making presentation and learning diaries. Below is presentation that was made using Xaringan about GOSAT-2 satellite.\n\n\n\n\n\n\n\n\nhttps://lamaals.github.io/remotely-sensing-presentation-/"
  },
  {
    "objectID": "week 3.html#summary",
    "href": "week 3.html#summary",
    "title": "3  Week 3 Correction: Refining the Lens",
    "section": "3.1 Summary",
    "text": "3.1 Summary\nIn this lecture, we had a look about the difference between Push broom and Whisk broom Sensors, then we discovered that satellite images usually might has flaws. Geometric Correction, Atmospheric Correction, Topographic Correction, and Radiometric are all methods to correct the remote sensing products. The next part of the lecture was about the joining data techniques and how to make enhancement on the images.\n\n3.1.1 Geometric Correction.\nThe process involves taking the coordinates of the dataset, applying a mathematical model to determine the transformation coefficients, and then using these coefficients to perform the geometric transformation, effectively altering the spatial arrangement of the features in the dataset. In other words it is the “ attempt to correct for positional errors and to transform the original image into a new image that has the geometric characteristics of a map. can you rephrase” I should add the citation (https://www.sciencedirect.com/topics/earth-and-planetary-sciences/geometric-correction#:~:text=Geometric%20corrections%20attempt%20to%20correct,geometric%20characteristics%20of%20a%20map.)\n\n\n\nFigure (1): Geometric Correction process\n\n\n\n\n3.1.2 Atmospheric Correction.\nAtmospheric correction is the process of eliminating scattering and absorption effects caused by the atmosphere, thereby revealing the surface reflectance that represents surface properties.\nMethods to remove the atmosphere effect:\nRelative: involves normalising images, which simplifies comparison with other images instead of directly eliminating atmospheric effects. One common technique for relative correction is Dark Object Subtraction (DOS), where dark objects with very low reflectance are subtracted from the image. This process not only standardizes the image but also mitigates atmospheric effects.\n\n\n\nFigure (2) Dark pixel subtraction source: (Remote Sensing Research center https://www.rsrc.org.au/)\n\n\nAbsolute: involves assessing atmospheric conditions, along with the angles of illumination and viewing during image acquisition, to estimate the levels of scattering and absorption for each image band. Based on these factors, correction factors are determined to adjust the data from its initial at-sensor values to more accurate at-surface values.\nEmpirical line method: is an atmospheric correction technique that offers an alternative to radiative transfer modeling approaches. It provides a relatively straightforward approach to calibrating surface reflectance, as long as a series of calibration target measurements that remain constant over time are accessible.\nPhases of Atmospheric Correction\nAtmospheric correction is a two-phase process. In the initial phase, Digital Numbers (DNs) are transformed into radiance, and subsequently into top-of-atmosphere radiance. In the subsequent phase, top-of-atmosphere reflectance is converted to surface reflectance, also known as bottom-of-atmosphere reflectance or top-of-canopy reflectance, particularly in vegetation studies. The output image from this process is referred to as atmospherically corrected.\n\n\n\nFigure (3) Phases of Atmospheric correction source: (https://www.linkedin.com/pulse/atmospheric-correction-dinesh-shrestha/)"
  },
  {
    "objectID": "week 3.html#application",
    "href": "week 3.html#application",
    "title": "3  Week 3 Correction: Refining the Lens",
    "section": "3.2 Application",
    "text": "3.2 Application\nApplying atmospheric corrections on satellite images is a crucial step that should be performed before conducting any further analysis or processing using those images. However, as we saw earlier, there are multiple approaches with different techniques that varies from image-based empirical correction to model-based methods. Each one of these techniques has its strength and weakness and, in this section, we will highlight two main studies that compared between different approaches like (QUAC, FLAASH and DOS). Those studies are “Comparison and evaluation of atmospheric correction algorithms of QUAC, DOS and FLAASH for HICO hyperspectral imagery” (Shi, Mao, 2016) and “A Comparison of Image-Based and Physics-Based Atmospheric Correction Methods for Extracting Snow and Vegetation Cover in Nepal Himalayas Using Landsat 8 OLI Images” (Niraj, Gupta 2022).\nComparison for HICO hyperspectral imagery paper:\nThis study concludes that all three types of corrections (FLAASH, QUAC, DOS) can remove the effect of atmosphere for HICO hyperspectral image. It also confirms that based on analysing the situ data, FLAASH model have a better performance than QUAC and DOS methods for reducing effect of atmosphere HICO images. Having said that, both QUAC and DOS depend on less input parameters and their computational speed is much faster than FLAASH.\n\n\n\n\n\nComparison for extracting snow and vegetation cover in Nepal Himalayas paper:\nIn this study, eight correction methods were applied on Landsat 8 OLI satellite image to find the best model for mapping snow and vegetation covered areas. The study found that FLAASH and 6SV methods determined best snow reflectance values, while DOS3 and QUAC were the worst. Additionally, FLAASH and SIAC methods showed greater vegetation reflectance values and higher ranks of correlating the extracted vegetation spectra with the standard spectra while DOS and QUAC were the lowest.\nThe study found when compared to other image-based correction methods (QUAC, Aref, COST, DOS, and DOS3), the FLAASH, SIAC, and 6SV methods generate higher snow and vegetation mean reflectance values, thus having a high possibility of mapping true snow and vegetation features.\n\n\n\n\n\n\n\n\n\n\n\n3.2.0.1 Conclusion\nBoth studies explore different atmospheric correction methods for remote sensing imagery, with a focus on varying types of imagery and target variables. The first study evaluates FLAASH, QUAC, and DOS correction methods for HICO hyperspectral imagery, concluding that FLAASH performs better in removing atmospheric effects despite its slower computational speed. In contrast, the second study assesses eight correction methods applied to Landsat 8 OLI satellite imagery to map snow and vegetation cover in the Nepal Himalayas, finding that FLAASH and SIAC methods produce superior results for snow and vegetation reflectance. Ultimately, it’s not possible to determine that one method is superior to another. Therefore, the selection of an atmospheric correction method relies on the specific objectives and characteristics of the remote sensing analysis."
  },
  {
    "objectID": "week 3.html#reflection",
    "href": "week 3.html#reflection",
    "title": "3  Week 3 Correction: Refining the Lens",
    "section": "3.3 Reflection",
    "text": "3.3 Reflection\nTo be honest, this lecture was quite challenging and lengthy, making it difficult to fully understand. Handling satellite images can indeed be tricky, yet mastering the art of manipulating them can prove invaluable for emphasizing specific points or shedding light on particular aspects. Understanding the various correction methods available is essential, as each method serves a distinct purpose. Moreover, selecting the appropriate method depends on the variables and characteristics of the analysis at hand. While I hope to avoid the need for performing corrections myself, as you mentioned, gaining insight into these behind-the-scenes processes has been enlightening and beneficial."
  },
  {
    "objectID": "week 3.html#reference",
    "href": "week 3.html#reference",
    "title": "3  Week 3 Correction: Refining the Lens",
    "section": "3.4 Reference",
    "text": "3.4 Reference\nWhat is Atmospheric Correction in Remote Sensing?\n(https://gisgeography.com/atmospheric-correction/)\nAtmospheric Correction\n(https://www.linkedin.com/pulse/atmospheric-correction-dinesh-shrestha/)\nRemote Sensing Research Center\n(https://sees-rsrc.science.uq.edu.au/rstoolkit/en/html/atmospheric/resources/fundamentals/corrections/physics-based.html#:~:text=Absolute%20atmospheric%20corrections,sensor%20to%20at%2Dsurface%20values.)\nComparison and evaluation of atmospheric correction algorithms of QUAC, DOS and FLAASH for HICO hyperspectral imagery(Shi, Mao, 2016)\n()\nA Comparison of Image-Based and Physics-Based Atmospheric Correction Methods for Extracting Snow and Vegetation Cover in Nepal Himalayas Using Landsat 8 OLI Images (Niraj, Gupta 2022).\n()"
  },
  {
    "objectID": "week 4.html#summary",
    "href": "week 4.html#summary",
    "title": "4  Week 4 policy: Transformative Shifts",
    "section": "4.1 Summary",
    "text": "4.1 Summary\nIn week 4, the primary emphasis was on utilizing remote sensing to accomplish policy objectives. We were tasked with selecting a city and identifying policy challenges that could be addressed using remotely sensed data.\nI am deeply intrigued by the observation and analysis of cities affected by conflict. I am convinced that leveraging satellite imagery to assess the economic, environmental, and social repercussions of war plays a vital role in formulating decisions that benefit the city’s well-being. This may help urban planners in implementing effective solutions for post-war urban development. Hence, I have chosen to study the impact of war on the socio-economic, environmental, and building damage aspects of Ukraine.\nAccording to United Nations Urban agenda:\n\nWe recognize that urban form, infrastructure and building design are among the greatest drivers of cost and resource efficiencies, through the benefits of economy of scale and agglomeration and by fostering energy efficiency, renewable energy, resilience, productivity, environmental protection and sustainable growth in the urban economy.\nsustainable management of natural resources in cities and human settlements in a manner that protects and improves the urban ecosystem and environmental services, reduces greenhouse gas emissions and air pollution and promotes disaster risk reduction and management,\n\nAlso highlighted in the Sustainable Development Goals agenda:\nGoal 11: Make cities and human settlements inclusive, safe, resilient and sustainable.In fact, sustainable development in cities is critical to achieving most of Agenda 2030."
  },
  {
    "objectID": "week 4.html#application",
    "href": "week 4.html#application",
    "title": "4  Week 4 policy: Transformative Shifts",
    "section": "4.2 Application",
    "text": "4.2 Application\nWars tend to have a series of effects and outcomes, the war on Ukraine has a massive impact on the economics of the country, just in the first year of the conflict Ukraine saw a devastating 30-35% decline in GDP, While there is a projected 0.5% GDP growth for 2023, it remains significantly below pre-war levels. The Ukraine economic deterioration is one of many side effects of war such as people displacement, infrastructure damage, pollution, and agriculture damage…. etc.\nThe focus of the Sustainable Development Goals (SDGs) and the new agenda is to prioritize the development of cities that exhibit resilience, environmental sustainability, and economic growth in urban areas. Therefore, it was appropriate to highlight Ukraine, a country in conflict, as a case study. This emphasis is necessary to navigate the crisis and ensure the well-being of its residents by fostering a post-war process that creates a more resilient city. This approach in Ukraine involves maintaining economic resilience, addressing environmental risks, and providing affordable housing for displaced people.\nThe use of remotely sensed data would be appropriate in this sensitive condition, as it can provide accurate information about the situation. This was shown by some of the studies conducted on Ukraine to monitor the socioeconomic change, air pollution, infrastructure damage, agriculture change assessment, and aid refugee relief operations. For this week, I will focus on a single paper while also recommending other remote sensing methods that could aid in studying the impact of wars. This paper details the method used for damage assessment : War Related Building Damage Assessment in Kyiv, Ukraine, Using Sentinel-1 Radar and Sentinel-2 Optical Images (Aimaiti, Koch 2022). In week 4 I’m only considering one paper but i will suggest other remote sensing methods that would help in studying the impact of wars.\nWar Related Building Damage Assessment in Kyiv, Ukraine, Using Sentinel-1 Radar and Sentinel-2 Optical Images:\nIn this study, medium-resolution satellite imagery was utilized because it is publicly available and capable of rapidly producing damage maps, which can serve as an initial reference for assessing damage.\nSentinel-1 and Sentinel-2 data were used assess building damage, a robust SAR log ratio of intensity method is utilized for Sentinel-1 data, and a texture analysis is conducted for Sentinel-2 data for damage assessment. To focus on urban areas and suppress changes from other features and landcover types, a built-up area mask is generated using OpenStreetMap building footprints and the World Settlement Footprint (WSF) dataset, respectively.\n\n4.2.0.1 Methods\nThe workflow consists of four phases: data selection and pre-processing, change detection analysis using the log ratio of intensity and Gray Level Co-occurrence Matrix (GLCM) methods, determination of an optimal threshold for separating changed and unchanged areas, and validation of results using high-resolution WorldView images and the UNOSAT damage assessment map.\n\n\n\nFigure (1): the processing workflow for building damage mapping using Sentinel-1 and Sentinel-2 images. Note: dif, AOI, and Tr refer to the difference, selected area for threshold determination, and threshold.\n\n\nSAR Intensity Analysis:\nA collapsed building exhibits a distinct backscatter structure compared to an intact one. Typically, strong backscattering diminishes or disappears after a building collapses due to a disaster. However, in partially damaged buildings, where sections of the wall and roof collapse, remaining walls, debris, and ground may form a corner reflector, resulting in strong double-bounce effects and increased backscattering intensity. This difference enables the identification of damaged and undamaged buildings using SAR backscattering intensity change detection. For this purpose, the study implemented the simple yet robust SAR log ratio of intensity method.\n\n\n\nFigure (2): schematic diagram of backscatter intensity from (a) intact buildings (b) destroyed buildings, and (c) partially damaged buildings in synthetic aperture radar (SAR) images; (a-1,a-2,b-1,b-2,c-1,c-2) are pre-and post-event Sentinel-1 images (19 February 2022 & 8 April 2022); (a-3,b-3,c-3) are corresponding WorldView images (25 & 31 March 2022).\n\n\nOptical Texture Analysis:\nIn this study, the GLCM Mean was utilized to differentiate between damaged and undamaged buildings, This feature is particularly valuable for classifying landscapes, especially in building damage assessment. GLCM Mean represents interior texture, characterized by high values in areas with subtle and irregular variations and few coherent edges. Statistics were calculated with a 3 × 3 window size using ENVI software. Initially, the average of pre-event images was computed to create a single pre-event texture image. Subsequently, the texture difference between this image and the post-event texture was calculated. The optimal threshold value for texture difference analysis was determined to be 0.4, with pixels below this threshold classified as damaged.\n\n\n\nFigure (3): the GLCM texture difference and the corresponding optical images. (a–d) are selected examples that have significant texture changes, and those locations are shown on the WorldView-2 image\n\n\n\n\n4.2.0.2 Results\nThe Sentinel-1 intensity analysis revealed flooding along the Irpin River. This area was excluded from the assessment as the flooding might be an indirect impact of war, but the focus is specifically on war-related damages to buildings and infrastructure. Significant building damage was concentrated in the northwestern part of Kyiv Oblast, particularly in Irpin, Bucha, and Hostomel. Building statistics, ranging from 85 m2 to 74,156 m2, were calculated using augmented OSM polygons and ArcGIS Desktop software. Most buildings identified by SAR intensity analysis were medium to large-scale structures. Smaller residential buildings were poorly detected, likely due to sensor limitations. High-rise buildings were flagged as damaged, but visual assessments using WorldView imagery did not confirm this finding. The WSF mask showed more false positives than the OSM mask.\n\n\n\nFigure (4): comparison of Sentinel-1 damaged building results using two different masks (OSM&WSF). (a) shows the SAR intensity-based results for the entire area. The locations (b–e) were selected as representative areas for comparison with the high-resolution WorldView image. The columns from left to right indicate the WorldView image (31 March 2022), OSM and WSF mask results of the damaged buildings. The scale shown in (b) is the same for (c–e).\n\n\nThe damage assessment results from the Sentinel-2 texture analysis revealed a flooded area along the Irpin River, similar to the SAR intensity analysis but with a smaller extent. Four representative areas were selected for comparison using the OSM mask versus the WSF built-up layer mask. The effects of these masks were less significant compared to the SAR intensity analysis, indicating fewer false positives in unmasked damage labels. Comparison with high-resolution WorldView images showed that the texture analysis mainly detected damages on large buildings with clear boundaries. However, buildings with dark roofs and smaller damaged buildings lacking distinct texture features compared to their surroundings were not captured in the analysis. Additionally, changes in intensity values of bright objects, representing either bare ground or tall structures showing different reflectance at different times, were misclassified as damaged buildings.\n\n\n\nFigure (5): the result of Sentinel-2 texture-based analysis for study region (a). (b–e) are comparison of Sentinel-2 damaged building results using two different masks (OSM&WSF). The locations (b–e) are referenced in (a) and were selected as representative areas for comparison with the high-resolution WorldView image. The columns from left to right indicate the WorldView image (25 March 2022 & 31 March 2022), OSM and WSF mask results of the damaged buildings. The scale shown in (b) is the same for (c–e).\n\n\n\n\n4.2.0.3 Conclusion\nThis study utilized Sentinel-1 and Sentinel-2 data to assess damage in Kyiv during the 2022 war. The analysis revealed concentrated damage in the northwestern Kyiv Oblast, particularly in cities like Bucha, Irpin, and Hostomel. When comparing damage detection algorithms, the analysis achieved 58% accuracy compared to UNOSAT verified damage, with 76% accuracy for larger buildings. The use of OpenStreetMap (OSM) and World Settlement Footprint (WSF) masks reduced false positives, but incomplete OSM footprints excluded some damage. Although texture analysis effectively detected large damaged buildings, it was less accurate for identifying small ones. Consequently, the study found that the intensity analysis performed better due to its ability to identify a wider range of building sizes accurately.\n\n\n4.2.0.4 Other methods that can be implemented in conflict zones using remote sensed data:\nEvaluation of Socio-Economic Consequences of war:\nThis can be done through the use of couple of methods like:\n\nusing satellite observations of nitrogen dioxide (NO2) and carbon dioxide (CO2) to monitor changes in human activities, such as fossil fuel combustion, power generation, and industrial production, during the conflict.\nDistinct fire and thermal anomaly patterns due to military activities, contrasting with past agricultural-related fires. These patterns, can be confirmed by Sentinel 2 imagery, carry significant socio-economic implications, straining recovery efforts and requiring resource allocation based on various factors.\nSatellite data from the VIIRS thermal imager utilization to monitor socio-economic developments, using Total Light Intensity (TLI) serving as an indicator of night illumination, reflecting economic activity, population density, and electricity consumption.\n\nMonitor environment and air quality:\n\nEmploying NO2 and CO satellite data, filtering the data fields using a quality assurance index and binned into regular grids for reliability and comparison.\nDetection of environmental changes: remote sensing enables the identification of alterations in natural settings, including deforestation, water pollution, shifts in vegetation cover, among others, potentially stemming from military activities.\n\nThese are several ways in which remotely sensed data can contribute to urban expansion and guide post-war urban planning initiatives to construct a more resilient city."
  },
  {
    "objectID": "week 4.html#reflection",
    "href": "week 4.html#reflection",
    "title": "4  Week 4 policy: Transformative Shifts",
    "section": "4.3 Reflection",
    "text": "4.3 Reflection\nConducting research on policies has deepened my understanding of how remote sensing data can drive transformation and development in urban areas, environmental monitoring, and socio-economic analysis, ultimately fostering economic growth. This comprehensive approach underscores the versatility of remote sensing and the various methods tailored to specific case studies. Despite its potential, remote sensing also poses limitations, such as restricted access to high-resolution data and potential inaccuracies in results. Nonetheless, I firmly believe that leveraging remote sensing methodologies and analyses can inform decision-making processes crucial for national and international development."
  },
  {
    "objectID": "week 4.html#reference",
    "href": "week 4.html#reference",
    "title": "4  Week 4 policy: Transformative Shifts",
    "section": "4.4 Reference",
    "text": "4.4 Reference\nWar Related Building Damage Assessment in Kyiv, Ukraine, Using Sentinel-1 Radar and Sentinel-2 Optical Images (Aimaiti, Koch 2022).\nSatellites capture socioeconomic disruptions during the 2022 full‑scale war in Ukraine (Ialongo, Hakkaraine 2022).\nUse of Satellite Information for Evaluation of Socio-Economic Consequences of the War in Ukraine (Yelistratova, L. O., Apostolov, O. A. 2022).\nMonitoring and assessment of the scale of destruction by remote sensing methods during the war in Ukraine (Ye. Butenko, S. Petrychenko 2023).\nRemotely visible impacts on air quality after a year-round full-scale Russian invasion of Ukraine (Savenets, Osadchyi 2023).\nUnited Nations New Urban Agenda\nUniversal Sustainable Development Goals"
  },
  {
    "objectID": "week 6.html#summary",
    "href": "week 6.html#summary",
    "title": "5  Week 6 GEE: Mapping the world",
    "section": "5.1 Summary",
    "text": "5.1 Summary\nThis week, we familiarized ourselves with Google Earth Engine (GEE) and its features, highlighting the advantages of using GEE and the various processes it facilitates. It became apparent early on in the lecture that using GEE would be more straightforward compared to QGIS and SNAP. This was not only a welcome relief but also an exciting prospect, given GEE’s reputation as a leading platform for analyzing remote sensing data.\n\n5.1.0.1 What is GEE?\nDue to the massive amount of data acquired from satellites, it became challenging to manage and analyze this data. Google Earth Engine (GEE) has emerged as a solution for processing and analyzing big data in this context. Essentially, GEE is a cloud-based platform capable of processing large geospatial datasets across extensive areas and monitoring environmental changes over extended periods."
  },
  {
    "objectID": "week 6.html#application",
    "href": "week 6.html#application",
    "title": "5  Week 6 GEE: Mapping the world",
    "section": "5.2 Application",
    "text": "5.2 Application\nGoogle Earth Engine is utilized in various applications and is widely employed globally for diverse studies and analyses. With that said, it is essential to briefly examine the significance of GEE, its functions, applications, advantages, and limitations in remote sensing analysis. This discussion will be based on two papers which are: Google Earth Engine Cloud Computing Platform for Remote Sensing Big Data Applications: A Comprehensive Review and Google Earth Engine for geo-big data applications: A meta-analysis and systematic review.\n\n5.2.0.1 Why Google earth engine.\nGEE platform allows users to tap into a vast collection of satellite images and geospatial data, spanning over a petabyte in size, all hosted on Google’s powerful computing infrastructure. This setup makes it ideal for conducting in-depth analysis on satellite imagery and remote sensing data. By integrating a comprehensive library of historical and up-to-date satellite imagery with robust APIs, Google Earth Engine empowers users to execute complex geospatial analyses directly in the cloud. This eliminates the need for users to download massive datasets to their local machines, facilitating the efficient processing of large-scale geospatial data. With its seamless access to such a wealth of geospatial information and the ability to perform sophisticated analyses in a cloud-based environment, Google Earth Engine stands out as an invaluable tool for researchers, scientists, and analysts working with satellite imagery and remote sensing data.\n\n\n5.2.0.2 GEE functions\nGoogle Earth Engine offers a wide range of functions for performing spectral and spatial operations on single or multiple images. While it supports various pixel-based spectral operations that can be efficiently implemented in parallel on cloud architecture, it has limited support for spatial functions such as filters, edge detection methods, line detection via Hough Transform, and morphological operators due to parallel implementation challenges. However, it provides access to supervised and unsupervised machine learning algorithms including CART, SVM, RF classifiers, and clustering algorithms like K-means, Cascade K-means, X-means, Cobweb, and SNIC for tasks like image classification and segmentation.With access to over 40 years of datasets, GEE enables temporal and change analyses using functions like CCDC, EWMACD, and LandTrendr. These functions facilitate tasks such as continuous change detection, trend analysis, and vegetation analysis. Specialized algorithms like VCT and VERDET are available for analyzing forest disturbances and vegetation changes over time.\n\n\n\nFigure (1): Different supporting functions within GEE.\n\n\n\n\n5.2.0.3 GEE Aplication\nThis platform presents a variety of applications, some of which will be depicted in the figure below along with the frequency of studies related to them. I will provide demonstrations for some of these applications.\n\nVegetation\n\nGEE’s computational efficiency supports large-scale and long-term vegetation monitoring, exemplified by studies mapping vegetation dynamics in Queensland, Australia, and detecting degradation in Rondônia, Brazil, with reported accuracies of 82.6% and 68.1%-85.3% respectively.\n\nUrban\n\nGEE facilitates long-term monitoring of urban dynamics, including expansion mapping, climate zone monitoring, 4-D modeling, green space classification, and heat island identification, with studies showcasing its efficiency and accuracy in assessing urban growth and Surface Urban Heat Island (SUHI) effects.\n\nLand Cover\n\nGEE provides extensive remote sensing datasets for land cover mapping, dynamics monitoring, coastal mapping, and wetland classification, with studies showcasing its effectiveness in regions like northern China and Mato Grosso, Brazil, achieving over 80% accuracy using various algorithms and satellite imagery integration.\n\nNatural disasters\n\nGEE facilitates real-time and long-term analysis of remotely sensed data, enabling monitoring, forecasting, and resilience studies of natural disasters that cause destruction to both environment and human life such as droughts, floods, wildfires, landslides, hurricanes, and tsunamis, with studies showcasing its effectiveness in regions like Punjab, Pakistan, and Bangladesh.\n\n\n\nFigure (2): applications of GEE\n\n\n\n\n5.2.0.4 Advantages and limitations\n\n\n\nFigure (3): Advantages and limitation of GEE source: (Google Earth Engine Cloud Computing Platform for Remote Sensing Big Data Applications: A Comprehensive Review)"
  },
  {
    "objectID": "week 6.html#reflection",
    "href": "week 6.html#reflection",
    "title": "5  Week 6 GEE: Mapping the world",
    "section": "5.3 Reflection",
    "text": "5.3 Reflection\nIt’s evident that Google Earth Engine (GEE) serves as a powerful tool for processing and analyzing large geospatial datasets, particularly in the realm of remote sensing. The platform’s cloud-based infrastructure and access to extensive satellite imagery and geospatial data make it invaluable for conducting comprehensive studies and analyses across various domains.Google Earth Engine plays a crucial role in advancing research and analysis in the field of remote sensing, offering powerful capabilities and a wide range of applications. Despite its limitations and it’s not widely used, GEE remains a valuable platform for researchers and analysts seeking to leverage geospatial data for various studies and analyses."
  },
  {
    "objectID": "week 6.html#reference",
    "href": "week 6.html#reference",
    "title": "5  Week 6 GEE: Mapping the world",
    "section": "5.4 Reference",
    "text": "5.4 Reference\nGoogle Earth Engine for geo-big data applications: A meta-analysis and systematic review. (Tamiminia H,Salehi B 2020)\nGoogle Earth Engine Cloud Computing Platform for Remote Sensing Big Data Applications: A Comprehensive Review."
  },
  {
    "objectID": "week 7.html#summary",
    "href": "week 7.html#summary",
    "title": "6  Week 7 Classification I: Navigating Machine learning",
    "section": "6.1 summary",
    "text": "6.1 summary\nDuring the lecture, we were introduced to classification using remotely-sensed data, exploring various approaches to applying classification and different methods for analysis using machine learning tools like CART and Random Forest. We also reviewed several papers previously examined in the course, discussing the methodologies employed in these studies. Following that, we delved into applying these methods to our own research studies.\n\n6.1.0.1 CART\nA binary decision classification tree method, simplifies decision-making through logical if-then scenarios. It recursively splits input data into groups based on predefined thresholds until reaching terminal nodes (Nods represent decision function ), with the most accurate tree selected. However, CART’s effectiveness can be limited by high dimensionality data.\n\n\n\nFigure (1): CART source: Medium\n\n\n\n\n6.1.0.2 Random Forest\nAn ensemble classifier that combines multiple CART trees. It generates decision trees using random selections of training datasets and variables, providing an unbiased assessment of generalization error. RF selects variables randomly at each node for tree building, with the optimal number of trees typically ranging from 100 to 500. The number of variables is determined by the square of the set of variables.\n\n\n\nFigure (2): Random Forest source: Medium\n\n\n\n\n6.1.0.3 Support Vector Machine(SVM)\nA supervised learning algorithm used for regression and classification problems. SVM constructs an ideal hyperplane during training to separate classes with the fewest misclassified pixels. It selects support vectors, extreme points that aid in hyperplane creation, using parameters like cost parameter C, Gamma, and kernel functions. Grid search is employed to define C and Gamma parameters, with C significantly influencing support vector selection and SVM performance. The linear kernel is preferred for training on large datasets.\n\n\n\nFigure (3): Support Vector Machine (SVM) source: Medium"
  },
  {
    "objectID": "week 7.html#application",
    "href": "week 7.html#application",
    "title": "6  Week 7 Classification I: Navigating Machine learning",
    "section": "6.2 Application",
    "text": "6.2 Application\nMachine learning is extensively employed across various domains including finance, trading technologies, healthcare, and traffic prediction. Exploring its application, particularly in image classification within remote sensing analysis using satellite images, is of great interest for me. Specifically focusing on urban environments, this section compares two studies: “Analysis of Land Use and Land Cover Using Machine Learning Algorithms on Google Earth Engine for Munneru River Basin, India” and “Mapping of Land Cover with Optical Images, Supervised Algorithms, and Google Earth Engine”. Both studies evaluate CART, Random Forest, and SVM algorithms for detecting land use and land cover (LULC) patterns.\nAnalysis of Land Use and Land Cover Using Machine Learning Algorithms on Google Earth Engine for Munneru River Basin, India:\nThis study aims to utilize machine learning algorithms on Google Earth Engine to classify land use and land cover (LULC) in the Munneru River Basin, India, comparing support vector machine (SVM), random forest (RF), and classification and regression trees (CART). Leveraging Earth observation data from Landsat-8 and Sentinel-2 satellite images, the analysis meticulously considers spatial resolutions and cloud cover criteria. Spectral bands from these satellites are used to classify LULC into five primary classes: water bodies, forests, barren lands, vegetation, and built-up areas. Orthorectified images with minimal cloud cover are processed using cloud mask algorithms, and yearly means of NDVI and NDWI indices are calculated to support classification. RF, CART, and SVM are then applied for comprehensive LULC analysis of the study area.\n\n6.2.0.1 Method\nOrthorectified images with minimal cloud cover served as the primary input for classification. Cloud shadow and cover were removed using a cloud mask technique, followed by eliminating contaminated pixels. Yearly means of normalized difference vegetation index (NDVI) and normalized difference water index (NDWI) were computed. Landsat and Sentinel data were merged into composite images using the median filter. Training polygons were generated from high-resolution Google Earth images, evenly distributed across five land use classes. These polygons were loaded into Google Earth Engine (GEE) as a feature collection table. Machine learning algorithms such as Random Forest (RF), Classification and Regression Trees (CART), and Support Vector Machine (SVM) were trained using Landsat-8 and Sentinel-2 images to classify land use and land cover (LULC).\n\n\n\nFigure (4): Methodology for LULC classification on the GEE platform.\n\n\n\n\n6.2.0.2 Discussion\nSVM, CART, and RF were employed to classify LULC using Landsat-8 and Sentinel-2 images on the GEE platform. Temporal aggregation methods were utilized to address gaps in cloudy images. NDWI and NDVI were utilized as additional inputs for LULC classification, representing water bodies and vegetation characteristics. A total of 575 training sites were utilized, with each class receiving 80-95 training samples and 65-80 validation samples. The best cross-validation factor for CART was determined to be 5 or 10. RF classification showed higher accuracy with a number of trees ranging from 50 to 100, with 100 trees yielding satisfactory results in this study. Important parameters such as kernel type, gamma value, and cost were considered in SVM classification. CART had a tendency to misclassify vegetation as built-up, water bodies, or forest in 2016 and 2018, and as barren land or water bodies in 202012. SVM slightly misclassified vegetation as forest, built-up, or water bodies in 2016 and as built-up or forest in 2018. However, SVM performed well in 2020, except for some forest and built-up areas. RF outperformed the other two classifiers in all three years.\n\n\n\nFigure (5): LULC maps of Landsat-8 images using SVM, RF, and CART classifiers for the years 2016, 2018, and 2020.\n\n\nIn terms of accuracy, RF outperformed SVM and CART for both Landsat-8 and Sentinel-2 images7. The overall accuracy for Landsat-8 was 94.85% for RF, 90.88% for SVM, and 82.88% for CART. For Sentinel-2, the overall accuracy was 95.84% for RF, 93.65% for SVM, and 86.48% for CART. The kappa coefficients, which measure the agreement between predicted and observed categorizations, were also highest for RF.\n\n\n\nFigure (6): Kappa coefficient and overall accuracy of Landsat-8 and Sentinel-2 for various machine learning classifiers.\n\n\nMapping of Land Cover with Optical Images, Supervised Algorithms, and Google Earth Engine:\nThe objective of this study is to evaluate the effectiveness of optical satellite images for land and land-cover mapping. The study area, situated in the eastern region of Tabasco, Mexico, covers towns like Balancán, Emiliano Zapata, and Tenosique. Characterized by abundant aquifers and sediment accumulation from streams, rivers, and lagoons, the area experiences a hot-humid climate. Utilizing Sentinel-2 satellite imagery via Google Earth Engine, the study established two annual time series spanning 2017 to 2019, aligning with crop cycles and regional weather patterns. The methodology involved mapping crops and land use using spectral indices and machine learning algorithms (SVM, RF, CART). Detailed tables depict land-use coverage across three zones for various seasons, showcasing changes in cropland, shrubland, water bodies, and more. Comparative analysis of classification errors for corn and sorghum crops using SVM, RF, and CART was conducted.\n\n\n6.2.0.3 Method\nIn the image selection phase, cloud masking using the QA60 band was employed to remove pixels with small accumulations of dense and cirrus clouds. A combination of reflectance thresholds and morphological operations was utilized to identify thick clouds and cirrus clouds, respectively. Then preprocessing phase involved calculating spectral indices for masked images, including NDVI, GNDVI, EVI, SAVI, and NDMI for vegetation detection, and NDWI for water bodies. Image correction techniques such as mosaicking and histogram reduction were applied to create mosaics of the study area, allowing for data aggregation over time. And the Supervised classification phase involved identifying main land types through visual analysis and applying RF, SVM, and CART algorithms to classify crops and soil types. Separate datasets were created for different crop cycles, and the dataset was divided into 70% for training and 30% for evaluation to avoid overtraining. SVM, RF, and CART algorithms were evaluated with different configurations to enhance classification efficiency.\n\n\n\nFigure (7): Proposed methodology for land-cover classification.\n\n\n\n\n6.2.0.4 Discussion\nThe SVM, RF, and CART classification algorithms were evaluated using different configurations on the Google Earth Engine (GEE) platform to enhance classification efficiency. For SVM, a kernel with a radial and gamma base function of 0.7 was employed, along with a cost of 30. Training occurred during both spring–summer and autumn–winter seasons. RF was configured to limit random forest trees to 20, minimizing misclassifications. The base GEE configuration was used for CART due to its lower classification error rate.\nTwo primary categories were defined: (1) crop types (including corn and sorghum) and (2) land use types (such as water bodies, urban areas, and tropical rainforest). The study assessed the accuracy of these classifications using overall training accuracy (OA) and the kappa index (KI). SVM performed exceptionally well, achieving an OA and KI of 0.996% in both seasons. RF also showed strong performance, with an OA and KI greater than 0.990 in spring–summer and 0.96% and 0.95% in autumn–winter. CART achieved an OA of 0.94% and a KI of 0.92% in the first season and 0.98% and 0.97% in the second season.\n\n\n\nFigure (8): Overall accuracy (OA) and Kappa index (KI) of the seasons.\n\n\nConsider the limitations of data sources. The SIAP collects crop data based on planted hectares, ignoring crops that do not sprout or grow. Consequently, the margins of error between the algorithm-detected hectares and SIAP data are substantial. While SVM outperformed actual data, there may still be errors due to occasional cultivation of small or intermittent crop lands.\n\n\n\nFigure (9): Percentage of corn and sorghum crop error by each classification method.\n\n\n\n\n6.2.0.5 Conclusion\nThe study methodology for land use and land cover (LULC) classification is outlined in both studiess, with the first one focusing on the preprocessing steps, feature selection, training dataset preparation, and classifier performance evaluation, particularly for SVM, CART, and RF classifiers. It provides detailed insights into the classification process, including input data selection, cloud masking techniques, temporal aggregation methods, and spectral index calculation. Moreover, it compares the classification results between the three classifiers, emphasizing the superior performance of RF over SVM and CART, this study presents accuracy assessment results using OA. The second study, meanwhile, discusses image selection, pre-processing, and supervised classification phases, detailing cloud masking, spectral index calculation, image correction, and classifier evaluation, with a focus on crop type classification. It also presents accuracy assessment results using OA and KI metrics. While both studies offer valuable insights, the first one provides a more comprehensive overview of the classification process and performance evaluation."
  },
  {
    "objectID": "week 7.html#reflection",
    "href": "week 7.html#reflection",
    "title": "6  Week 7 Classification I: Navigating Machine learning",
    "section": "6.3 Reflection",
    "text": "6.3 Reflection\nIn my journey of learning about machine learning applications in remote sensing, I initially found the concept quite hard. However, as I delved into research papers and explored the topic further, things started to fall into place. I was particularly intrigued to see how concepts we covered in lectures, like image correction and spectral index, were put into action combained with machine learning in real-world scenarios. For example, I learned about CART’s method of dividing data into groups based on predefined thresholds, which, although simple, is limited by high-dimensional data. On the other hand, RF, with its ensemble approach combining multiple CART trees, stood out for its ability to improve accuracy while reducing overfitting, although it requires more computational resources. SVM also caught my attention for its ability to create an optimal hyperplane for class separation with minimal misclassification, especially in datasets with many dimensions. However, I noted that it can be computationally intensive and sensitive to parameter tuning. Overall, I realized that choosing between these methods depends on various factors, such as the nature of the dataset, desired interpretability, available computational resources, and the trade-offs between accuracy and complexity."
  },
  {
    "objectID": "week 7.html#reference",
    "href": "week 7.html#reference",
    "title": "6  Week 7 Classification I: Navigating Machine learning",
    "section": "6.4 Reference",
    "text": "6.4 Reference\nAnalysis of Land Use and Land Cover Using Machine Learning Algorithms on Google Earth Engine for Munneru River Basin, India (Loukika K, Keesara V 2021).\nMapping of Land Cover with Optical Images, Supervised Algorithms, and Google Earth Engine (Pech-May F, Aquino-Santos R 2022)."
  },
  {
    "objectID": "week 8.html#summary",
    "href": "week 8.html#summary",
    "title": "7  week 8 classification:",
    "section": "7.1 Summary",
    "text": "7.1 Summary\nThis lecture builds upon the foundational concepts covered in last week’s lecture, which focused on CART, Random Forest, and SVM. However, this time, we’ll cover additional principles such as Object-Based analysis, Sub-Pixel analysis, cross validation, and Spatial cross validation but because object-based analysis and sub-pixel analysis are new to me I will talk about them in my diary.\nObject-Based Image analysis\nObject-based image analysis (OBIA) involves grouping pixels into objects based on spectral similarity or external variables like ownership or soil type. These objects have various attributes such as spectral, shape, and neighborhood characteristics. OBIA allows for the inclusion of landscape knowledge by applying rules to classify objects. For example, identifying a group of trees, grass, and water near dense housing as a city park or distinguishing between forest and individual trees. Compared to traditional spectral image analysis, OBIA offers improved accuracy and detail in classification.\n\n\n\nFigure (1): Object-based Analysis\n\n\nSub-Pixel analysis\nSub-pixel analysis in remote sensing involves examining the spectral composition within individual pixels to detect subtle changes that may not be apparent at the pixel level. By analysing the fractions or proportions of different land cover types within a single pixel, known as endmembers, using advanced algorithms like spectral unmixing, sub-pixel analysis offers a more detailed understanding of landscape dynamics, especially in areas where different land cover types are mixed within a pixel. This approach enhances change detection accuracy and sensitivity. Sub-pixel processing addresses the possibility of a pixel belonging to different classes in an image segmentation context, increasing the resolution of original images."
  },
  {
    "objectID": "week 8.html#application",
    "href": "week 8.html#application",
    "title": "7  week 8 classification:",
    "section": "7.2 Application",
    "text": "7.2 Application\nFor the application part and after giving a brief about Sub-Pixel analysis and Image-based analysis I will be comparing two studies where each one of them used one of the previous method these two studies are: Sub-pixel change detection for urban land-cover analysis via multi-temporal remote sensing images and Object-based land cover change detection for cross-sensor images\nSub-pixel change detection for urban land-cover analysis via multi-temporal remote sensing images:\n\n7.2.0.1 Method:\nThe proposed method employs an unmixing algorithm to ascertain the proportions of endmembers within a pixel, based on the V-I-S model. Unlike traditional methods yielding binary change results, sub-pixel analysis delves into the variability within pixels, considering all endmembers. Decision-level fusion techniques are used to integrate differential information and determine changes, including class transitions, direction, and intensity. The method involves four key steps: spectral unmixing, differential information generation, change determination based on defined rules, and analysis of change intensity.\n\n\n7.2.0.2 Steps:\n\nSpectral Unmixing:\n\nInitially, the BPNN unmixing algorithm is employed to produce the abundance of each endmember within a single pixel in images captured at two different dates.\n\nDifferential Information Generation:\n\nThe discrepancy in abundance for all endmembers within a pixel between two dated images is computed, comprising K fractions represented as Dk.\n\nChange Determination:\n\nDetermining change information at the sub-pixel level involves analysing the change indicator to identify changed pixels. This process entails thresholding the change magnitude image of all pixels to create a change map.\n\nChange Intensity Analysis:\n\nChange intensity, a measure of change probability, aids in detecting potential changes. After identifying changed pixels, intensity is categorized into different levels. Higher intensity values identify significant changes, indicating a greater likelihood of real changes within urban areas.\n\n\n\nFigure (2): Flowchart of the proposed sub-pixel level change detection approach.\n\n\n\n\n7.2.0.3 Experiment:\nThe study conducted experiments primarily focusing on multi-temporal China-Brazil earth resources satellite images of Shanghai city. These images, captured on March 7, 2005, and May 7, 2009, covered a 1000 × 1000 pixel area, including urban and Pudong New District. Land-cover changes observed mainly involved built-up areas and vegetation, with minor changes in soil and water. Abundance maps of different end members were analyzed for both dates. The proposed method demonstrated higher overall accuracy (89.86%) and kappa coefficient (0.7791) compared to other methods like CVA and PCA, with notable reductions in commission and omission rates. This suggests the effectiveness of the proposed approach in accurately detecting land-cover changes over time.\n\n\n7.2.0.4 Results and Findings of the Experiment:\n\nThe proposed method yields dependable results, aligning with ground truth data and photo interpretation. It offers detailed insights such as change transition, direction, and intensity to decision-makers. Changes are quantitatively evaluated, enhancing the delineation of relevant change areas. Additionally, the change intensity analysis provides rich supplementary information, particularly highlighting high probability regions, which closely correspond to actual changes.\nThe change matrix illustrates the transitions in land cover among different categories. Analysis reveals that between 2005 and 2009, land-cover alterations predominantly involve shifts from low-albedo to high-albedo (5498 pixels), soil to high-albedo (4157 pixels), low albedo to soil (4008 pixels), and soil to vegetation (2806 pixels). These findings underscore the significant influence of urbanization on land-cover changes in urban regions, particularly evident in transitions from low-albedo to high-albedo and soil to vegetation.\nThe findings obtained through sub-pixel level detection offer a higher level of completeness and accuracy compared to pixel-level techniques, offering a wealth of change information to aid decision-making and field assessments. However, methods such as CVA and PCA-based approaches exhibit some errors, notably omission errors, leading to a reduction in overall change detection accuracy.\n\n\n\n\nFigure (3): Binary change detection maps obtained by different approaches: (a) The proposed method; (b) CVA; and (c) PCA.\n\n\n\n\n\nFigure (4): Accuracy and errors of change detection approaches.\n\n\nObject-based land cover change detection for cross-sensor images:\nThe study focuses on Daqing, located in northeastern China’s Heilongjiang province, known for its diverse land cover and significant land cover changes from the 1960s to the 1990s due to petroleum exploitation. However, environmental quality has improved since the 1990s due to the local government’s eco-friendly initiatives. The study used Landsat 5 TM and IRS-P6 LISS3 images from 1990 and 2006, respectively, for land cover change detection, supplemented with QuickBird images, land use survey maps, field trips, and interviews for data collection and validation.\n\n\n7.2.0.5 Method\nThe suggested method involves preprocessing of data, segmenting the images, classifying objects, and accuracy assessment.\nImage preprocessing and transformation\nTo ensure compatibility between images, a subset of the IRS image overlapping with the TM image is chosen as T2 and georeferenced to it with an RMSE of under 0.6 pixels. T2’s pixel resolution is adjusted to match the TM images at 30 meters. Similarly, a subset of the TM images overlapping with T2 is selected as T1. Image transformation methods have been used in land cover change detection to reduce redundancy and enhance features. This study introduces a novel approach allowing for classification just once. Eigenvalue analysis suggests that the first six bands contain the most significant information, hence they are utilized for subsequent analysis.\nObject-based classification\n\nImage segmentation:\n\nImage segmentation is vital for analyzing remote sensing data. Definiens Professional offers multi-resolution segmentation, a technique merging regions from one-pixel objects upward, considering both pixel value and texture. In this study, it’s applied using the first six bands of a PCA-transformed image. Parameters like homogeneity, shape, and compactness are adjusted based on visual inspection, resulting in successful partitioning of land cover change patches into image objects.\n\nImage–object classification:\n\nObject-based image classification aims to assign components to specific categories. However, conventional classification schemes struggle to capture dynamic land cover changes. A new classification scheme is devised, describing changes between land cover types. Significant spectral and textural differences between images, especially in water areas, prompt the inclusion of subclasses based on water color. Visual interpretation of images aids in selecting 247 objects as samples for training and validation. Object-based classification often employs non-parametric algorithms due to the complexity of image objects’ attributes. In this study, NN classification is applied based on spectral attributes and textural parameters, utilizing the digital number (DN) values of six principal bands for segmentation. The shape of image segments is disregarded as it doesn’t provide significant information related to land cover change types.\n\n\n\nFigure (5): Workflow of the proposed approach.\n\n\n\n\n7.2.0.6 Results\nDue to challenges in achieving precise registration accuracy using pixel-based image analysis, especially with cross-sensor images of varying resolutions, alternative validation strategies are needed. In this study, 62 validation polygons comprising 7710 pixels are chosen through visual interpretation to assess object-based classification accuracy. The below table illustrates the confusion matrix as well as detailed accuracy information which reveals highest producer’s accuracies for water-to-water and farmland-to-builtup classes. However, errors occur in distinguishing wetland-to-water and water-to-wetland changes, likely due to spatial confusion during image segmentation. Similarly, confusion between wetland-to-builtup and farmland-to-builtup changes is observed due to spectral similarities. Despite some errors, overall accuracy and kappa coefficient are satisfactory at 83.42% and 0.82, respectively, indicating the effectiveness of the proposed approach in land cover change detection.\n\n\n\nFigure (6): Confusion matrix and accuracy of the land cover change map.\n\n\nThe figure depicts the land cover changes in the study area from 1990 to 2006, presenting both current and dynamic land cover information. Visual analysis reveals that the predominant change is ‘farmland-to-farmland’, indicating sustained farmland preservation. Additionally, there’s no notable decline in wetlands, suggesting their conservation during the period. Despite rapid economic growth, urban sprawl appears limited, with few areas transitioning from farmland to urban. Overall, the map indicates effective protection of farmland and wetlands amidst China’s urbanization, highlighting successful land management strategies in the region.\n\n\n\nFigure (7): Land cover change map with vector layers: water lines include rivers and irrigation channels; the major roads are highways which cross the study area.\n\n\n\n\n7.2.0.7 Conclusion\nBoth studies propose distinct methodologies for analyzing land cover changes using remote sensing data. Study 1 focuses on sub-pixel level change detection, employing an unmixing algorithm to ascertain endmember proportions within pixels and decision-level fusion techniques to identify changes based on defined criteria. Results from experiments on multi-temporal China-Brazil earth resources satellite images demonstrate reliable change detection, providing detailed change information and achieving higher accuracy compared to traditional methods. On the other hand, Study 2 introduces a novel object-based classification approach involving preprocessing, image segmentation, and non-parametric classification algorithms. Despite challenges in distinguishing certain change types, the method demonstrates satisfactory accuracy and offers insights into land cover changes over time. While Study 1 offers more detailed change information, Study 2’s approach may be more suitable for analyzing complex land cover changes. Ultimately, the choice between the two methods depends on the specific requirements and objectives of the analysis."
  },
  {
    "objectID": "week 8.html#reflection",
    "href": "week 8.html#reflection",
    "title": "7  week 8 classification:",
    "section": "7.3 Reflection",
    "text": "7.3 Reflection"
  },
  {
    "objectID": "week 8.html#reference",
    "href": "week 8.html#reference",
    "title": "7  week 8 classification:",
    "section": "7.4 Reference",
    "text": "7.4 Reference\nSub-pixel change detection for urban land-cover analysis via multi-temporal remote sensing images.(DU P, LIU S 2014)\nObject-based land cover change detection for cross-sensor images. ( Y. Qin, Z. Niu 2013).\nGIM International (https://www.gim-international.com/content/article/object-based-image-analysis)"
  },
  {
    "objectID": "week 9.html#summary",
    "href": "week 9.html#summary",
    "title": "8  week 9 SAR:",
    "section": "8.1 Summary",
    "text": "8.1 Summary\nIn today’s lecture, we explored Synthetic Aperture Radar (SAR) technology and compared it with the optical imagery we’ve been using throughout this module. We delved into the details of SAR, including its polarization and wavelength, and discussed how SAR functions. Specifically, we focused on change detection in SAR images. While we discussed one method in depth during the lecture, the slides presented various other methods for change detection.\nSynthetic Aperture Radar (SAR)\n(SAR) is a radar technology utilized for generating two-dimensional images or three-dimensional reconstructions of various objects, including landscapes. SAR achieves finer spatial resolution by employing the motion of the radar antenna over a target area, which contrasts with the stationary beam-scanning radars traditionally used. Functioning as an active sensor, SAR transmits microwave signals and captures the signals reflected, or backscattered, from the Earth’s surface. This capability enables SAR to produce high-resolution images using relatively compact antennas. Notably, SAR possesses unique attributes such as cloud-penetrating capabilities.\n\n\n\nFigure (1): Synthetic Aperture Radar (SAR)\n\n\nSAR Polarization and Scattering Mechanisms\nPolarization refers to the orientation of the plane in which the transmitted electromagnetic wave swings. SAR sensors usually transmit linearly polarized whereby the advantage of radar sensors is that signal polarization can be precisely controlled on both transmit and receive. Signal strength of different polarizations carries information about the imaged surface structure, based on different types of scattering which are rough surface, volume, and double bounce scattering.\n\nRough surface scattering, such as that caused by bare soil or water, is most sensitive to VV scattering.\nVolume scattering, for example, caused by the leaves and branches in a forest canopy, is most sensitive to cross-polarized data like VH or HV.\nThe last type of scattering, double bounce, is caused by buildings, tree trunks, or inundated vegetation and is most sensitive to an HH polarized signal.\n\nLastly, as wavelength changes signal penetration depth, it drives the amount of signal attributed to different scattering types.\n\n\n\nFigure (2): SAR Polarization and Scattering Source: NASA Earth Data\n\n\nChange Detection in SAR\nChange detection in (SAR) images involves identifying changes in land cover over time by examining and contrasting two or more SAR images captured at different times but covering the same geographic region. This method is highly beneficial in remote sensing applications due to SAR’s ability to acquire images regardless of weather conditions and its capability to penetrate clouds and darkness. This technique is of high practical value to a large number of applications, such as flood detection, disaster monitoring, urban planning, and land cover data monitoring. However, the inherent speckle noise in SAR images can lead to false alarms and misdetections."
  },
  {
    "objectID": "week 9.html#application",
    "href": "week 9.html#application",
    "title": "8  week 9 SAR:",
    "section": "8.2 Application",
    "text": "8.2 Application\nIn this application section, I’ll delve into the study of change detection in SAR images. While we’ve explored various techniques and methods for change detection in previous lectures, I’ve chosen to focus on a methodology that hasn’t been extensively covered. My aim is to introduce a new approach that offers insights beyond what’s been discussed in class, enriching my understanding and exploration of SAR image analysis for change detection purposes in my latest learning diary entry.\nChange Detection in Synthetic Aperture Radar Images based on Image Fusion and Fuzzy Clustering:\nThis paper highlights the challenges in change detection in Synthetic Aperture Radar (SAR) images, primarily due to speckle noise. The paper suggests the use of the ratio operator over subtraction for SAR images to handle speckle noise and calibration errors. To enhance change detection, it introduces image fusion to combine mean-ratio and log-ratio images. Furthermore, it proposes a robust fuzzy clustering algorithm that incorporates spatial context to improve resistance against noise and enhance change detection accuracy.\nMethodology\nFor the methodology they proposed a novel approach for change detection in SAR images, comprising two main steps: generating a difference image through image fusion and detecting changed areas in the fused image using an improved Fuzzy C-Means (FCM) algorithm:\n\nImage Fusion techniques are employed to enhance image quality, primarily focusing on pixel-level fusion using methods like the discrete wavelet transform (DWT). The DWT efficiently isolates frequencies in time and space, making it suitable for change detection tasks, particularly with large volumes of image data. The fusion process involves computing the DWT of each source image, fusing corresponding coefficients, and applying inverse DWT to obtain the fused result.\nFuzzy Clustering: An improved FCM algorithm is utilized for clustering to discriminate changed and unchanged areas in the fused image. Traditional FCM algorithms often lack robustness to noise, prompting the development of variants like FCM_S and FGFCM, which incorporate local spatial information. However, these approaches require parameter tuning and may still be sensitive to noise. To address this, a robust Fuzzy Local Information C-Means (FLICM) algorithm is proposed, which employs a fuzzy factor to balance noise robustness and image detail preservation. Further improvement is made by replacing spatial distance with the local coefficient of variation to better handle noise and preserve image details, resulting in the reformulated RFLICM algorithm.\n\n\n\n\nFigure (3): Flowchart of the proposed change detection approach.\n\n\n\n8.2.0.1 Experiments\nTo assess the effectiveness of the proposed analyzing method they ran experiments on three different data sets. I will discuss in detail one of them as I believe it is adequate for this section:\n\nBern data set\n\nThe efficacy of wavelet image fusion in generating difference images was assessed against mean-ratio and log-ratio methods. The fused difference image effectively captured step changes, suppressing unchanged regions using wavelet coefficients from the log-ratio image. Comparing performance, the log-ratio operator achieved a PCC of 99.27% for Otsu and 99.24% for K-means, while the proposed approach attained the highest PCC (99.35% for Otsu and 99.36% for K-means) and kappa (0.781 for Otsu and 0.784 for K-means). Visual analysis revealed more spots in change detection from the mean-ratio image due to speckle effects, while the log-ratio image had fewer spots but suffered from information loss in changed areas. The wavelet fusion image notably reduced errors in change detection results. In the second experiment, the impact of the RFLICM algorithm on SAR-image change detection, based on wavelet fusion difference images, was assessed. Traditional FCM produced many spots due to its lack of consideration for spatial context. However, FLICM and RFLICM, incorporating local information, yielded robust change detection maps, with RFLICM outperforming FLICM and FCM with a PCC of 99.68%, 99.66%, and 99.37% respectively.\n\n\n\nFigure (4): Change detection results of the Bern data set based on the three difference images obtained by Otsu. (a) Based on the mean-ratio operator. (b) Based on the log-ratio operator. (c) Based on wavelet fusion.\n\n\n\n\n\nFigure (5): Change detection results of the Bern data set based on the three difference images obtained by K-means. (a) Based on the mean-ratio operator. (b) Based on the log-ratio operator. (c) Based on wavelet fusion.\n\n\n\n\n\nFigure (6): Change detection results of the Bern data set achieved by (a) FCM, (b) FLICM, and (c) proposed RFLICM.\n\n\n\n\n8.2.0.2 Conclusion\nThe method used in change detection in (SAR) images, which combines wavelet image fusion and an improved fuzzy clustering algorithm, Robust Fuzzy Local Information C-Means (RFLICM). This method stands out from other methods like image differencing technique which often struggle due to the presence of speckle noise and the multiplicative nature of speckles, as well as the statistical properties of SAR images being non-robust to calibration errors. The wavelet fusion approach in the proposed method enhances changed region information and suppresses background noise by integrating mean-ratio and log-ratio images. Meanwhile, the RFLICM algorithm incorporates local spatial and gray information, making it more precise and less sensitive to the probability statistics model than traditional thresholding techniques. Experimental results show that this method outperforms traditional ones, with the wavelet fusion strategy integrating the advantages of log-ratio and mean-ratio operators, and RFLICM exhibiting fewer errors and spots in change detection results. I really like this approach because and according to my understanding this can be useful in providing a more accurate reflection for real world changes and better suppression of background noise."
  },
  {
    "objectID": "week 9.html#reflection",
    "href": "week 9.html#reflection",
    "title": "8  week 9 SAR:",
    "section": "8.3 Reflection",
    "text": "8.3 Reflection\nIt was really interesting to cover and go deep in the change detection of SAR images methods. For me I feel like because I’m interested in using sattelite images and remote sensing to map the impact of conflict and to detect changes in those areas. In the lectures we talked about images enhancements methods and image fusion was one of them. I got intrigued in finding out more about image fusion methods especially the ones that we haven’t covered. For this reason, I searched for discrete wavelet transform, which was surprisingly an eye opening because I learned so much about the SAR images change detection and what are the hardships in its applications. I think this change detaction method is crucial for humanitarian organization work in detecting in a precise manner where to focus their efforts and the most damaged areas and buildings. This will eventually lead to deliver the most efficent excution plan for any humanitarian intervention. Some of the stuff that are covered in this lecture was mentioned before but briefly like polarization and scattering although they are simple but they have a great importance."
  },
  {
    "objectID": "week 9.html#reference",
    "href": "week 9.html#reference",
    "title": "8  week 9 SAR:",
    "section": "8.4 Reference",
    "text": "8.4 Reference\nChange Detection in Synthetic Aperture Radar Images based on Image Fusion and Fuzzy Clustering (Gong.M, Zhou.Z 2012)\nChange Detection on SAR Images (R.A. Alagu Raja,K. Vaiyammal 2017).\nDeep Learning-Based Suppression of Speckle-Noise in Synthetic Aperture Radar (SAR) Images: A Comprehensive Review (Kant Shukla.S, K. Dwivedi.S 2023).\nOverview of SAR Image Change Detection (Xuan.J, Xin.Z 2021)\nDeep Despeckling of SAR Images to Improve Change Detection Performance (Ihmeida.M, Shahzad.M 2023).\nChange Detection from Synthetic Aperture Radar Images via Graph-Based Knowledge Supplement Network (Wang.J, Gao.F 2022)\nUnsupervised SAR Image Change Detection Based on Structural Consistency and CFAR Threshold Estimation (Zhu.J, Wang.F 2023)\nhttps://asf.alaska.edu/information/sar-information/what-is-sar/\nhttps://www.earthdata.nasa.gov/learn/backgrounders/what-is-sar\nhttps://www.earthdata.nasa.gov/learn/backgrounders/what-is-sar"
  }
]